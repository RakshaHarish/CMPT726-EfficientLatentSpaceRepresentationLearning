{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQwMu-5txHur"
      },
      "source": [
        "import torch, torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# dataset to be finalised\n",
        "# assumption -> image is 28x28, input vector size = 784 -> to be changed depending on the final dataset\n",
        "\n",
        "# Note : to be filled\n",
        "# batch_size = \n",
        "# train =\n",
        "# train_loader =\n",
        "# test =  \n",
        "# test_loader =\n",
        "\n",
        "# initialize random seeds; select gpu device if available\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1) \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N36829QyfMh"
      },
      "source": [
        "# function to display the reconstructed images\n",
        "# this function can be called during training/testing\n",
        "def display_images(in_, out, n=1, label=None, count=False):\n",
        "    for N in range(n):\n",
        "        if in_ is not None:\n",
        "            in_pic = in_.data.cpu().view(-1, 28, 28)\n",
        "            plt.figure(figsize=(20, 4))\n",
        "            plt.suptitle(label + ' â€“ reconstructed images', color='w', fontsize=20)\n",
        "            for i in range(4):\n",
        "                plt.subplot(1,4,i+1)\n",
        "                plt.imshow(in_pic[i+4*N])\n",
        "                plt.axis('off')\n",
        "        out_pic = out.data.cpu().view(-1, 28, 28)\n",
        "        plt.figure(figsize=(20, 6))\n",
        "        for i in range(4):  \n",
        "            plt.subplot(1,4,i+1)\n",
        "            plt.imshow(out_pic[i+4*N])\n",
        "            plt.axis('off')\n",
        "            if count: plt.title(str(4 * N + i), color='w')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYcioy6ysbx"
      },
      "source": [
        "# creating basic VAE model for image reconstruction; depth encoding - pending\n",
        "\n",
        "d = 10 # data points can be varied accordingly\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(nn.Linear(784, d ** 2),nn.ReLU(),nn.Linear(d ** 2, d * 2))\n",
        "        self.decoder = nn.Sequential(nn.Linear(d, d ** 2),nn.ReLU(),nn.Linear(d ** 2, 784),nn.Sigmoid(),)\n",
        "        \n",
        "        # Note : classes for Encoder/Decoder can be added to encode depths using convolutions\n",
        "        # class Encoder(nn.Module):\n",
        "        # class Decoder(nn.Module):\n",
        "\n",
        "    def reparam(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()  # compute standard dev using non-negative logvariance\n",
        "            eps = std.data.new(std.size()).normal_() # to get normal distribution for backprop in training\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu_logvar = self.encoder(x.view(-1, 784)).view(-1, 2, d)\n",
        "        mu = mu_logvar[:, 0, :]\n",
        "        logvar = mu_logvar[:, 1, :]  # calculating the value of 'mu' and 'logvar' from encoder\n",
        "        z = self.reparam(mu, logvar) # finding z by reparameterization\n",
        "        return self.decoder(z), mu, logvar # return the decoder output\n",
        "\n",
        "model = VAE().to(device)\n",
        "\n",
        "# Setting the optimiser\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01) # learning rate can be modified accordingly\n",
        "\n",
        "# define the loss function -> using binary_cross_entropy loss with KL divergence\n",
        "def loss_function(x_hat, x, mu, logvar):  # where x = instance of training; x_hat, mu, logvar = model(x); same concept can be applied for testing phase\n",
        "    BCE = nn.functional.binary_cross_entropy(x_hat, x.view(-1, 784), reduction='sum')\n",
        "    KLDiv = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
        "    return BCE + KLDiv\n",
        "\n",
        "#write code to train and test the model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}