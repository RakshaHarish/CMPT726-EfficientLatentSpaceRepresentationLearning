{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYU_rgbd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQwMu-5txHur"
      },
      "source": [
        "import torch, torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "# NYU Dataset \n",
        "# -> image is 216x216, input vector size = 46656  \n",
        "\n",
        "class NYU_DepthDataset(Dataset):\n",
        "    def __init__(self, mat_file):\n",
        "      mat = loadmat(mat_file)\n",
        "      images = torch.from_numpy(mat['images']).permute(3, 2, 0, 1)\n",
        "      depths = torch.from_numpy(mat['depths']).permute(2, 0, 1)\n",
        "\n",
        "      images_max = images.amax((2, 3), keepdim=True) \n",
        "      images_min = images.amin((2, 3), keepdim=True) \n",
        "      images = (images - images_min) / (images_max - images_min)\n",
        "\n",
        "      depths_max = depths.amax((1, 2), keepdim=True) \n",
        "      depths_min = depths.amin((1, 2), keepdim=True) \n",
        "      depths = (depths - depths_min) / (depths_max - depths_min)\n",
        "\n",
        "      self.rgbd = torch.cat((images, depths.unsqueeze(1)), dim=1)\n",
        "\n",
        "      self.std, self.mean = torch.std_mean(self.rgbd, (0, 2, 3))\n",
        "      self.rgbd = transforms.functional.normalize(self.rgbd, self.mean, self.std)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.rgbd.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.rgbd[idx]\n",
        "\n",
        "# Note : to be filled\n",
        "batch_size = 50\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "nyu = NYU_DepthDataset('drive/My Drive/Colab Notebooks/nyu.mat')\n",
        "nyu_train = Subset(nyu, range(0, 1159))\n",
        "nyu_test = Subset(nyu, range(1159, len(nyu)))\n",
        "trainloader = DataLoader(dataset=nyu_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=torch.cuda.is_available())\n",
        "testloader = DataLoader(dataset=nyu_test, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=torch.cuda.is_available())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg3fN8H4Q6W0",
        "outputId": "c9ace41b-a958-47bd-94bc-b5142f3a78fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apDNQOfWoIzp"
      },
      "source": [
        "# initialize random seeds; select gpu device if available\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1) \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N36829QyfMh"
      },
      "source": [
        "# function to display the reconstructed images\n",
        "# this function can be called during training/testing\n",
        "def display_images(in_, out, n=1, label=None, count=False):\n",
        "    for N in range(n):\n",
        "        if in_ is not None:\n",
        "            in_pic = in_.data.cpu().view(-1, 216, 216)\n",
        "            plt.figure(figsize=(20, 4))\n",
        "            plt.suptitle(label + ' â€“ reconstructed images', color='w', fontsize=20)\n",
        "            for i in range(4):\n",
        "                plt.subplot(1,4,i+1)\n",
        "                plt.imshow(in_pic[i+4*N])\n",
        "                plt.axis('off')\n",
        "        out_pic = out.data.cpu().view(-1, 216, 216)\n",
        "        plt.figure(figsize=(20, 6))\n",
        "        for i in range(4):  \n",
        "            plt.subplot(1,4,i+1)\n",
        "            plt.imshow(out_pic[i+4*N])\n",
        "            plt.axis('off')\n",
        "            if count: plt.title(str(4 * N + i), color='w')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYcioy6ysbx"
      },
      "source": [
        "# creating basic VAE model for image reconstruction; depth encoding - pending\n",
        "\n",
        "d = 10 # data points can be varied accordingly\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(nn.Linear(46656, d ** 2),nn.ReLU(),nn.Linear(d ** 2, d * 2))\n",
        "        self.decoder = nn.Sequential(nn.Linear(d, d ** 2),nn.ReLU(),nn.Linear(d ** 2, 46656),nn.Sigmoid(),)\n",
        "        \n",
        "        # Note : classes for Encoder/Decoder can be added to encode depths using convolutions\n",
        "        # class Encoder(nn.Module):\n",
        "        # class Decoder(nn.Module):\n",
        "\n",
        "    def reparam(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()  # compute standard dev using non-negative logvariance\n",
        "            eps = std.data.new(std.size()).normal_() # to get normal distribution for backprop in training\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu_logvar = self.encoder(x.view(-1, 46656)).view(-1, 2, d)\n",
        "        mu = mu_logvar[:, 0, :]\n",
        "        logvar = mu_logvar[:, 1, :]  # calculating the value of 'mu' and 'logvar' from encoder\n",
        "        z = self.reparam(mu, logvar) # finding z by reparameterization\n",
        "        return self.decoder(z), mu, logvar # return the decoder output\n",
        "\n",
        "model = VAE().to(device)\n",
        "\n",
        "# Setting the optimiser\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001) # learning rate can be modified accordingly\n",
        "\n",
        "# define the loss function -> using binary_cross_entropy loss with KL divergence\n",
        "def loss_function(x_hat, x, mu, logvar):  # where x = instance of training; x_hat, mu, logvar = model(x); same concept can be applied for testing phase\n",
        "    BCE = nn.functional.binary_cross_entropy(x_hat, x.view(-1, 46656), reduction='sum')\n",
        "    KLDiv = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
        "    return BCE + KLDiv\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siZs8R5ilURa",
        "outputId": "b7240bcd-53dc-4266-acd4-a891336a3b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Training and testing the VAE\n",
        "\n",
        "epochs = 20 # set higher epoch values - around 300 to 500\n",
        "codes = dict(mu1=list(), var1=list()) #, y1=list())\n",
        "for epoch in range(0, epochs + 1):\n",
        "    # Training\n",
        "    if epoch > 0:  # test untrained net first\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x in trainloader:\n",
        "            x = x.to(device)\n",
        "            x_hat, mu, logvar = model(x)\n",
        "            loss = loss_function(x_hat, x, mu, logvar)\n",
        "            train_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "       \n",
        "        print(f'====> Epoch: {epoch} Train loss: {train_loss / len(train_loader.dataset):.4f}')\n",
        "    \n",
        "    # Testing\n",
        "    means, logvars, labels = list(), list(), list()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        for x in testloader:\n",
        "            x = x.to(device)\n",
        "            x_hat, mu, logvar = model(x)\n",
        "            test_loss += loss_function(x_hat, x, mu, logvar).item()\n",
        "            means.append(mu.detach())\n",
        "            logvars.append(logvar.detach())\n",
        "            #labels.append(y.detach())\n",
        "     \n",
        "    codes['mu1'].append(torch.cat(means))\n",
        "    codes['var1'].append(torch.cat(logvars))\n",
        "    #codes['y1'].append(torch.cat(labels))\n",
        "    test_loss /= len(testloader.dataset)\n",
        "    print(f'====> Test set loss: {test_loss:.4f}')\n",
        "    display_images(x, x_hat, 1, f'Epoch {epoch}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            
              ],
          "name": "stdout"
        }
      ]
    }
  ]
}
